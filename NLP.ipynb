{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading word: Package 'word' not found in index\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Hritik\n",
      "[nltk_data]     Kounsal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Importing the relevent libraries.\n",
    "import bs4 as bs\n",
    "import urllib.request\n",
    "import re\n",
    "import math\n",
    "import pandas as pd\n",
    "import nltk, string, numpy as np\n",
    "nltk.download('word')\n",
    "nltk.download('punkt')\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using text mining for extraction of data from source.\n",
    "def Extracter(url):\n",
    "    scraped_data = urllib.request.urlopen(url)\n",
    "    article = scraped_data.read()\n",
    "\n",
    "    parsed_article = bs.BeautifulSoup(article,'lxml')\n",
    "\n",
    "    paragraphs = parsed_article.find_all(id='p-2')\n",
    "\n",
    "    article_text = \"\"\n",
    "\n",
    "    for p in paragraphs:\n",
    "        article_text += p.text\n",
    "    return article_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "para1 = Extracter('https://www.medrxiv.org/content/10.1101/2020.02.24.20027052v1')\n",
    "para2 = Extracter('https://www.medrxiv.org/content/10.1101/2020.03.07.20031575v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABSTRACT # 1:  Objective: To investigate the correlation between clinical characteristics and cardiac injury of COVID-2019 pneumonia. \n",
      "Methods: In this retrospective, single-center study, 41 consecutive corona virus disease 2019 (COVID-2019) patients (including 2 deaths) of COVID-2019 in Beijing Youan Hospital, China Jan 21 to Feb 03, 2020, were involved in this study. The high risk factors of cardiac injury in different COVID-2019 patients were analyzed. Computed tomographic (CT) imaging of epicardial adipose tissue (EAT) has been used to demonstrate the cardiac inflammation of COVID-2019.\n",
      "Results：Of the 41 COVID-2019 patients, 2 (4.88%), 32 (78.05%), 4 (9.75%) and 3 (7.32%) patients were clinically diagnosed as light, mild, severe and critical cases, according to the 6th guidance issued by the National Health Commission of China. 10 (24.4%) patients had underlying complications, such as hypertension, CAD, type 2 diabetes mellites and tumor. The peak value of TnI in critical patients is 40-fold more than normal value. 2 patients in the critical group had the onset of atrial fibrillation, and the peak heart rates reached up to 160 bpm. CT scan showed low EAT density in severe and critical patients. \n",
      "Conclusion: Our results indicated that cardiac injury of COVID-2019 was rare in light and mild patients, while common in severe and critical patients. Therefore, the monitoring of the heart functions of COVID-2019 patients and applying potential interventions for those with abnormal cardiac injury related characteristics, is vital to prevent the fatality.\n",
      "\n",
      "-----------\n",
      "\n",
      "ABSTRACT # 2:  The outbreak of coronavirus disease 2019 (COVID-19) caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) in China has been declared a public health emergency of international concern. The cardiac injury was dominate in the process. However, whether N terminal pro B type natriuretic peptide (NT-proBNP) predicted outcome of COVID-19 patients was unknown. The study initially enrolled 102 patients with severe COVID-19 pneumonia from a continuous sample. After screening out the ineligible cases, 54 patients were analyzed in this study. Results found that patients with higher NT-proBNP (above 88.64 pg/mL) level had more risks of in-hospital death. After adjusting for potential cofounders in separate modes, NT-proBNP presented as an independent risk factor of in-hospital death in patients with severe COVID-19.\n",
      "\n",
      "-----------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('ABSTRACT # 1: ', para1)\n",
    "\n",
    "print('\\n-----------\\n')\n",
    "\n",
    "print('ABSTRACT # 2: ',para2)\n",
    "\n",
    "print('\\n-----------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing the paragraphs extracted to the notepad files.\n",
    "def fileWrite(filename, content):\n",
    "    abs_file = open(filename, 'w', encoding='utf-8')\n",
    "    abs_file.write(content)\n",
    "fileWriter(r\"C:\\Users\\Hritik Kounsal\\Desktop\\NLP1.TXT\", para1)\n",
    "fileWriter(r\"C:\\Users\\Hritik Kounsal\\Desktop\\NLP2.TXT\", para2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = para1\n",
    "p2 = para2\n",
    "doc = [p1, p2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using nltk for tokenization of words from paragraphs.\n",
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['objective',\n",
       " 'to',\n",
       " 'investigate',\n",
       " 'the',\n",
       " 'correlation',\n",
       " 'between',\n",
       " 'clinical',\n",
       " 'characteristic',\n",
       " 'and',\n",
       " 'cardiac',\n",
       " 'injury',\n",
       " 'of',\n",
       " 'covid2019',\n",
       " 'pneumonia',\n",
       " 'method',\n",
       " 'in',\n",
       " 'this',\n",
       " 'retrospective',\n",
       " 'singlecenter',\n",
       " 'study',\n",
       " '41',\n",
       " 'consecutive',\n",
       " 'corona',\n",
       " 'virus',\n",
       " 'disease',\n",
       " '2019',\n",
       " 'covid2019',\n",
       " 'patient',\n",
       " 'including',\n",
       " '2',\n",
       " 'death',\n",
       " 'of',\n",
       " 'covid2019',\n",
       " 'in',\n",
       " 'beijing',\n",
       " 'youan',\n",
       " 'hospital',\n",
       " 'china',\n",
       " 'jan',\n",
       " '21',\n",
       " 'to',\n",
       " 'feb',\n",
       " '03',\n",
       " '2020',\n",
       " 'were',\n",
       " 'involved',\n",
       " 'in',\n",
       " 'this',\n",
       " 'study',\n",
       " 'the',\n",
       " 'high',\n",
       " 'risk',\n",
       " 'factor',\n",
       " 'of',\n",
       " 'cardiac',\n",
       " 'injury',\n",
       " 'in',\n",
       " 'different',\n",
       " 'covid2019',\n",
       " 'patient',\n",
       " 'were',\n",
       " 'analyzed',\n",
       " 'computed',\n",
       " 'tomographic',\n",
       " 'ct',\n",
       " 'imaging',\n",
       " 'of',\n",
       " 'epicardial',\n",
       " 'adipose',\n",
       " 'tissue',\n",
       " 'eat',\n",
       " 'ha',\n",
       " 'been',\n",
       " 'used',\n",
       " 'to',\n",
       " 'demonstrate',\n",
       " 'the',\n",
       " 'cardiac',\n",
       " 'inflammation',\n",
       " 'of',\n",
       " 'covid2019',\n",
       " 'results：of',\n",
       " 'the',\n",
       " '41',\n",
       " 'covid2019',\n",
       " 'patient',\n",
       " '2',\n",
       " '488',\n",
       " '32',\n",
       " '7805',\n",
       " '4',\n",
       " '975',\n",
       " 'and',\n",
       " '3',\n",
       " '732',\n",
       " 'patient',\n",
       " 'were',\n",
       " 'clinically',\n",
       " 'diagnosed',\n",
       " 'a',\n",
       " 'light',\n",
       " 'mild',\n",
       " 'severe',\n",
       " 'and',\n",
       " 'critical',\n",
       " 'case',\n",
       " 'according',\n",
       " 'to',\n",
       " 'the',\n",
       " '6th',\n",
       " 'guidance',\n",
       " 'issued',\n",
       " 'by',\n",
       " 'the',\n",
       " 'national',\n",
       " 'health',\n",
       " 'commission',\n",
       " 'of',\n",
       " 'china',\n",
       " '10',\n",
       " '244',\n",
       " 'patient',\n",
       " 'had',\n",
       " 'underlying',\n",
       " 'complication',\n",
       " 'such',\n",
       " 'a',\n",
       " 'hypertension',\n",
       " 'cad',\n",
       " 'type',\n",
       " '2',\n",
       " 'diabetes',\n",
       " 'mellites',\n",
       " 'and',\n",
       " 'tumor',\n",
       " 'the',\n",
       " 'peak',\n",
       " 'value',\n",
       " 'of',\n",
       " 'tni',\n",
       " 'in',\n",
       " 'critical',\n",
       " 'patient',\n",
       " 'is',\n",
       " '40fold',\n",
       " 'more',\n",
       " 'than',\n",
       " 'normal',\n",
       " 'value',\n",
       " '2',\n",
       " 'patient',\n",
       " 'in',\n",
       " 'the',\n",
       " 'critical',\n",
       " 'group',\n",
       " 'had',\n",
       " 'the',\n",
       " 'onset',\n",
       " 'of',\n",
       " 'atrial',\n",
       " 'fibrillation',\n",
       " 'and',\n",
       " 'the',\n",
       " 'peak',\n",
       " 'heart',\n",
       " 'rate',\n",
       " 'reached',\n",
       " 'up',\n",
       " 'to',\n",
       " '160',\n",
       " 'bpm',\n",
       " 'ct',\n",
       " 'scan',\n",
       " 'showed',\n",
       " 'low',\n",
       " 'eat',\n",
       " 'density',\n",
       " 'in',\n",
       " 'severe',\n",
       " 'and',\n",
       " 'critical',\n",
       " 'patient',\n",
       " 'conclusion',\n",
       " 'our',\n",
       " 'result',\n",
       " 'indicated',\n",
       " 'that',\n",
       " 'cardiac',\n",
       " 'injury',\n",
       " 'of',\n",
       " 'covid2019',\n",
       " 'wa',\n",
       " 'rare',\n",
       " 'in',\n",
       " 'light',\n",
       " 'and',\n",
       " 'mild',\n",
       " 'patient',\n",
       " 'while',\n",
       " 'common',\n",
       " 'in',\n",
       " 'severe',\n",
       " 'and',\n",
       " 'critical',\n",
       " 'patient',\n",
       " 'therefore',\n",
       " 'the',\n",
       " 'monitoring',\n",
       " 'of',\n",
       " 'the',\n",
       " 'heart',\n",
       " 'function',\n",
       " 'of',\n",
       " 'covid2019',\n",
       " 'patient',\n",
       " 'and',\n",
       " 'applying',\n",
       " 'potential',\n",
       " 'intervention',\n",
       " 'for',\n",
       " 'those',\n",
       " 'with',\n",
       " 'abnormal',\n",
       " 'cardiac',\n",
       " 'injury',\n",
       " 'related',\n",
       " 'characteristic',\n",
       " 'is',\n",
       " 'vital',\n",
       " 'to',\n",
       " 'prevent',\n",
       " 'the',\n",
       " 'fatality']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Output of tokens generated from para1.\n",
    "LemNormalize(para1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "LemVectorizer = CountVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
    "LemVectorizer.fit_transform(doc)\n",
    "sparse_matrix = LemVectorizer.fit_transform(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'objective': 111,\n",
       " 'investigate': 94,\n",
       " 'correlation': 51,\n",
       " 'clinical': 38,\n",
       " 'characteristic': 36,\n",
       " 'cardiac': 33,\n",
       " 'injury': 91,\n",
       " 'covid2019': 53,\n",
       " 'pneumonia': 119,\n",
       " 'method': 102,\n",
       " 'retrospective': 134,\n",
       " 'singlecenter': 143,\n",
       " 'study': 144,\n",
       " '41': 13,\n",
       " 'consecutive': 47,\n",
       " 'corona': 49,\n",
       " 'virus': 156,\n",
       " 'disease': 63,\n",
       " '2019': 5,\n",
       " 'patient': 115,\n",
       " 'including': 84,\n",
       " '2': 4,\n",
       " 'death': 56,\n",
       " 'beijing': 30,\n",
       " 'youan': 159,\n",
       " 'hospital': 81,\n",
       " 'china': 37,\n",
       " 'jan': 97,\n",
       " '21': 7,\n",
       " 'feb': 71,\n",
       " '03': 0,\n",
       " '2020': 6,\n",
       " 'involved': 95,\n",
       " 'high': 79,\n",
       " 'risk': 135,\n",
       " 'factor': 69,\n",
       " 'different': 62,\n",
       " 'analyzed': 26,\n",
       " 'computed': 44,\n",
       " 'tomographic': 149,\n",
       " 'ct': 55,\n",
       " 'imaging': 83,\n",
       " 'epicardial': 68,\n",
       " 'adipose': 24,\n",
       " 'tissue': 147,\n",
       " 'eat': 65,\n",
       " 'ha': 76,\n",
       " 'used': 154,\n",
       " 'demonstrate': 58,\n",
       " 'inflammation': 88,\n",
       " 'results：of': 133,\n",
       " '488': 14,\n",
       " '32': 10,\n",
       " '7805': 18,\n",
       " '4': 11,\n",
       " '975': 20,\n",
       " '3': 9,\n",
       " '732': 17,\n",
       " 'clinically': 39,\n",
       " 'diagnosed': 61,\n",
       " 'light': 99,\n",
       " 'mild': 103,\n",
       " 'severe': 141,\n",
       " 'critical': 54,\n",
       " 'case': 34,\n",
       " 'according': 22,\n",
       " '6th': 16,\n",
       " 'guidance': 75,\n",
       " 'issued': 96,\n",
       " 'national': 107,\n",
       " 'health': 77,\n",
       " 'commission': 41,\n",
       " '10': 1,\n",
       " '244': 8,\n",
       " 'underlying': 152,\n",
       " 'complication': 43,\n",
       " 'hypertension': 82,\n",
       " 'cad': 32,\n",
       " 'type': 151,\n",
       " 'diabetes': 60,\n",
       " 'mellites': 101,\n",
       " 'tumor': 150,\n",
       " 'peak': 116,\n",
       " 'value': 155,\n",
       " 'tni': 148,\n",
       " '40fold': 12,\n",
       " 'normal': 109,\n",
       " 'group': 74,\n",
       " 'onset': 112,\n",
       " 'atrial': 28,\n",
       " 'fibrillation': 72,\n",
       " 'heart': 78,\n",
       " 'rate': 128,\n",
       " 'reached': 129,\n",
       " '160': 3,\n",
       " 'bpm': 31,\n",
       " 'scan': 138,\n",
       " 'showed': 142,\n",
       " 'low': 100,\n",
       " 'density': 59,\n",
       " 'conclusion': 46,\n",
       " 'result': 132,\n",
       " 'indicated': 86,\n",
       " 'wa': 158,\n",
       " 'rare': 127,\n",
       " 'common': 42,\n",
       " 'monitoring': 105,\n",
       " 'function': 73,\n",
       " 'applying': 27,\n",
       " 'potential': 120,\n",
       " 'intervention': 93,\n",
       " 'abnormal': 21,\n",
       " 'related': 130,\n",
       " 'vital': 157,\n",
       " 'prevent': 123,\n",
       " 'fatality': 70,\n",
       " 'outbreak': 113,\n",
       " 'coronavirus': 50,\n",
       " 'covid19': 52,\n",
       " 'caused': 35,\n",
       " 'acute': 23,\n",
       " 'respiratory': 131,\n",
       " 'syndrome': 145,\n",
       " 'sarscov2': 137,\n",
       " 'declared': 57,\n",
       " 'public': 126,\n",
       " 'emergency': 66,\n",
       " 'international': 92,\n",
       " 'concern': 45,\n",
       " 'dominate': 64,\n",
       " 'process': 125,\n",
       " 'n': 106,\n",
       " 'terminal': 146,\n",
       " 'pro': 124,\n",
       " 'b': 29,\n",
       " 'natriuretic': 108,\n",
       " 'peptide': 117,\n",
       " 'ntprobnp': 110,\n",
       " 'predicted': 121,\n",
       " 'outcome': 114,\n",
       " 'unknown': 153,\n",
       " 'initially': 90,\n",
       " 'enrolled': 67,\n",
       " '102': 2,\n",
       " 'continuous': 48,\n",
       " 'sample': 136,\n",
       " 'screening': 139,\n",
       " 'ineligible': 87,\n",
       " '54': 15,\n",
       " 'higher': 80,\n",
       " '8864': 19,\n",
       " 'pgml': 118,\n",
       " 'level': 98,\n",
       " 'inhospital': 89,\n",
       " 'adjusting': 25,\n",
       " 'cofounder': 40,\n",
       " 'separate': 140,\n",
       " 'mode': 104,\n",
       " 'presented': 122,\n",
       " 'independent': 85}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The vocabulary generated from both paragraphs.\n",
    "LemVectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  1  0  1  4  1  1  1  1  1  1  1  1  2  1  0  1  1  1  0  1  1  1  0\n",
      "   1  0  1  1  1  0  1  1  1  5  1  0  2  2  1  1  0  1  1  1  1  0  1  1\n",
      "   0  1  0  1  0  8  5  2  1  0  1  1  1  1  1  1  0  2  0  0  1  1  1  1\n",
      "   1  1  1  1  1  1  2  1  0  1  1  1  1  0  1  0  1  0  0  4  0  1  1  1\n",
      "   1  1  0  2  1  1  1  2  0  1  0  1  0  1  0  1  1  0  0 11  2  0  0  1\n",
      "   1  0  0  1  0  0  0  1  1  1  1  0  1  1  1  1  0  0  1  0  0  3  1  1\n",
      "   2  0  0  1  1  1  1  1  1  0  1  2  1  1  1  1]\n",
      " [ 0  0  1  0  1  1  0  0  0  0  0  0  0  0  0  1  0  0  0  1  0  0  0  1\n",
      "   0  1  1  0  0  1  0  0  0  1  1  1  0  1  0  0  1  0  0  0  0  1  0  0\n",
      "   1  0  2  0  4  0  0  0  2  1  0  0  0  0  0  1  1  0  1  1  0  1  0  0\n",
      "   0  0  0  0  1  1  0  0  1  0  0  0  0  1  0  1  0  2  1  1  1  0  0  0\n",
      "   0  0  1  0  0  0  0  0  1  0  1  0  1  0  3  0  0  1  1  5  0  1  1  1\n",
      "   1  1  1  0  1  1  1  0  0  0  0  1  1  0  0  2  1  1  0  1  1  3  0  0\n",
      "   2  1  1  0  0  0  0  1  0  1  0  0  0  0  2  0]]\n"
     ]
    }
   ],
   "source": [
    "#Using tf-idf function from sklearn.\n",
    "tf_matrix = LemVectorizer.transform(doc).toarray()\n",
    "print (tf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.40546511 1.40546511 1.40546511 1.40546511 1.         1.\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.         1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.         1.         1.40546511\n",
      " 1.40546511 1.         1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.         1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.         1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.         1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.         1.\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.         1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.         1.40546511 1.40546511 1.40546511 1.\n",
      " 1.         1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.         1.40546511 1.40546511 1.         1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.         1.40546511 1.40546511\n",
      " 1.         1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.         1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.         1.40546511]\n"
     ]
    }
   ],
   "source": [
    "tfidfTran = TfidfTransformer(norm=\"l2\")\n",
    "tfidfTran.fit(tf_matrix)\n",
    "print (tfidfTran.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The idf for terms that appear in one document: 1.916290731874155\n",
      "The idf for terms that appear in two documents: 1.5108256237659907\n"
     ]
    }
   ],
   "source": [
    "#mathematical computation idf.\n",
    "def idf(n,df):\n",
    "    result = math.log((n+1.0)/(df+1.0)) + 1\n",
    "    return result\n",
    "print( \"The idf for terms that appear in one document: \" + str(idf(4,1)))\n",
    "print( \"The idf for terms that appear in two documents: \" + str(idf(4,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.05636107 0.05636107 0.         0.05636107 0.16040545 0.04010136\n",
      "  0.05636107 0.05636107 0.05636107 0.05636107 0.05636107 0.05636107\n",
      "  0.05636107 0.11272213 0.05636107 0.         0.05636107 0.05636107\n",
      "  0.05636107 0.         0.05636107 0.05636107 0.05636107 0.\n",
      "  0.05636107 0.         0.04010136 0.05636107 0.05636107 0.\n",
      "  0.05636107 0.05636107 0.05636107 0.20050681 0.04010136 0.\n",
      "  0.11272213 0.08020272 0.05636107 0.05636107 0.         0.05636107\n",
      "  0.05636107 0.05636107 0.05636107 0.         0.05636107 0.05636107\n",
      "  0.         0.05636107 0.         0.05636107 0.         0.45088853\n",
      "  0.28180533 0.11272213 0.04010136 0.         0.05636107 0.05636107\n",
      "  0.05636107 0.05636107 0.05636107 0.04010136 0.         0.11272213\n",
      "  0.         0.         0.05636107 0.04010136 0.05636107 0.05636107\n",
      "  0.05636107 0.05636107 0.05636107 0.05636107 0.04010136 0.04010136\n",
      "  0.11272213 0.05636107 0.         0.05636107 0.05636107 0.05636107\n",
      "  0.05636107 0.         0.05636107 0.         0.05636107 0.\n",
      "  0.         0.16040545 0.         0.05636107 0.05636107 0.05636107\n",
      "  0.05636107 0.05636107 0.         0.11272213 0.05636107 0.05636107\n",
      "  0.05636107 0.11272213 0.         0.05636107 0.         0.05636107\n",
      "  0.         0.05636107 0.         0.05636107 0.05636107 0.\n",
      "  0.         0.44111499 0.11272213 0.         0.         0.04010136\n",
      "  0.04010136 0.         0.         0.05636107 0.         0.\n",
      "  0.         0.05636107 0.05636107 0.05636107 0.05636107 0.\n",
      "  0.04010136 0.05636107 0.05636107 0.04010136 0.         0.\n",
      "  0.05636107 0.         0.         0.12030409 0.05636107 0.05636107\n",
      "  0.08020272 0.         0.         0.05636107 0.05636107 0.05636107\n",
      "  0.05636107 0.04010136 0.05636107 0.         0.05636107 0.11272213\n",
      "  0.05636107 0.05636107 0.04010136 0.05636107]\n",
      " [0.         0.         0.09717175 0.         0.0691385  0.0691385\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.09717175 0.         0.\n",
      "  0.         0.09717175 0.         0.         0.         0.09717175\n",
      "  0.         0.09717175 0.0691385  0.         0.         0.09717175\n",
      "  0.         0.         0.         0.0691385  0.0691385  0.09717175\n",
      "  0.         0.0691385  0.         0.         0.09717175 0.\n",
      "  0.         0.         0.         0.09717175 0.         0.\n",
      "  0.09717175 0.         0.19434349 0.         0.38868699 0.\n",
      "  0.         0.         0.138277   0.09717175 0.         0.\n",
      "  0.         0.         0.         0.0691385  0.09717175 0.\n",
      "  0.09717175 0.09717175 0.         0.0691385  0.         0.\n",
      "  0.         0.         0.         0.         0.0691385  0.0691385\n",
      "  0.         0.         0.09717175 0.         0.         0.\n",
      "  0.         0.09717175 0.         0.09717175 0.         0.19434349\n",
      "  0.09717175 0.0691385  0.09717175 0.         0.         0.\n",
      "  0.         0.         0.09717175 0.         0.         0.\n",
      "  0.         0.         0.09717175 0.         0.09717175 0.\n",
      "  0.09717175 0.         0.29151524 0.         0.         0.09717175\n",
      "  0.09717175 0.34569249 0.         0.09717175 0.09717175 0.0691385\n",
      "  0.0691385  0.09717175 0.09717175 0.         0.09717175 0.09717175\n",
      "  0.09717175 0.         0.         0.         0.         0.09717175\n",
      "  0.0691385  0.         0.         0.138277   0.09717175 0.09717175\n",
      "  0.         0.09717175 0.09717175 0.20741549 0.         0.\n",
      "  0.138277   0.09717175 0.09717175 0.         0.         0.\n",
      "  0.         0.0691385  0.         0.09717175 0.         0.\n",
      "  0.         0.         0.138277   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "tfidf_matrix = tfidfTran.transform(tf_matrix)\n",
    "print (tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.        0.2772548]\n",
      " [0.2772548 1.       ]]\n"
     ]
    }
   ],
   "source": [
    "#Using cosine similarity to check between 2 paragraphs.\n",
    "cos_similarity_matrix = (tfidf_matrix * tfidf_matrix.T).toarray()\n",
    "print (cos_similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p1</th>\n",
       "      <th>p2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>p1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.277255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p2</th>\n",
       "      <td>0.277255</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          p1        p2\n",
       "p1  1.000000  0.277255\n",
       "p2  0.277255  1.000000"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(cos_similarity_matrix, index= ['p1','p2'],columns=['p1','p2'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
